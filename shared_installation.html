<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>CANDLE Shared Installation &mdash; CANDLE  documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="How to Contribute" href="contribute.html" />
    <link rel="prev" title="candle" href="candle/candle.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> CANDLE
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="readme.html">What is CANDLE?</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="candle/candle.html"><code class="docutils literal notranslate"><span class="pre">candle</span></code></a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">CANDLE Shared Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#terminology-and-scope">Terminology and scope</a></li>
<li class="toctree-l2"><a class="reference internal" href="#overview-of-wrapper-scripts-functionality">Overview of wrapper scripts functionality</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#for-users">For users</a></li>
<li class="toctree-l3"><a class="reference internal" href="#for-developers">For developers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#loading-the-candle-module">Loading the <code class="docutils literal notranslate"><span class="pre">candle</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quick-start-examples-for-summit">Quick-start examples (for Summit)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-setup">Step 1: Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-run-sample-candle-compliant-model-scripts">Step 2: Run sample CANDLE-compliant model scripts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#nt3-using-upf-candle-compliant-model-scripts">NT3 using UPF (CANDLE-compliant model scripts)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nt3-using-mlrmbo-candle-compliant-model-scripts">NT3 using mlrMBO (CANDLE-compliant model scripts)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-run-sample-non-candle-compliant-model-scripts">Step 3: Run sample <strong>non</strong>-CANDLE-compliant model scripts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mnist-using-upf-non-candle-compliant-model-scripts">MNIST using UPF (non-CANDLE-compliant model scripts)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nt3-using-mlrmbo-non-candle-compliant-model-scripts">NT3 using mlrMBO (non-CANDLE-compliant model scripts)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-minimally-modify-a-bare-model-script-for-use-with-the-wrapper-scripts">How to minimally modify a bare model script for use with the wrapper scripts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#running-a-non-candle-compliant-model-on-its-own-outside-of-supervisor">Running a non-CANDLE-compliant model on its own, outside of Supervisor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#input-file-format">Input file format</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#control-section"><code class="docutils literal notranslate"><span class="pre">&amp;control</span></code> section</a></li>
<li class="toctree-l3"><a class="reference internal" href="#default-model-section"><code class="docutils literal notranslate"><span class="pre">&amp;default_model</span></code> section</a></li>
<li class="toctree-l3"><a class="reference internal" href="#param-space-section"><code class="docutils literal notranslate"><span class="pre">&amp;param_space</span></code> section</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#code-organization">Code organization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#recommendations-for-particular-use-cases">Recommendations for particular use cases</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#run-grid-or-bayesian-hyperparameter-searches-on-an-already-candle-compliant-model-script-such-as-a-benchmark">Run <code class="docutils literal notranslate"><span class="pre">grid</span></code> or <code class="docutils literal notranslate"><span class="pre">bayesian</span></code> hyperparameter searches on an already CANDLE-compliant model script such as a benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="#create-a-new-model-script-on-which-you-want-to-run-grid-or-bayesian-hyperparameter-searches">Create a new model script on which you want to run <code class="docutils literal notranslate"><span class="pre">grid</span></code> or <code class="docutils literal notranslate"><span class="pre">bayesian</span></code> hyperparameter searches</a></li>
<li class="toctree-l3"><a class="reference internal" href="#run-a-model-script-written-in-another-language-such-as-r-or-bash">Run a model script written in another language such as <code class="docutils literal notranslate"><span class="pre">R</span></code> or <code class="docutils literal notranslate"><span class="pre">bash</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pull-updates-to-the-central-installation-of-candle-that-have-already-been-pulled-into-the-main-supervisor-benchmarks-repositories">Pull updates to the central installation of CANDLE that have already been pulled into the main Supervisor/Benchmarks repositories</a></li>
<li class="toctree-l3"><a class="reference internal" href="#commit-changes-to-the-wrapper-scripts-or-to-the-supervisor-or-benchmarks-clones-in-the-central-installation">Commit changes to the wrapper scripts or to the Supervisor or Benchmarks clones in the central installation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#contribution-ideas">Contribution ideas</a></li>
<li class="toctree-l2"><a class="reference internal" href="#known-issues">Known issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-contact-andrew-for-help-with-anything-above">How to contact Andrew for help with anything above</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contribute.html">How to Contribute</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">CANDLE</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>CANDLE Shared Installation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/shared_installation.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="candle-shared-installation">
<h1>CANDLE Shared Installation<a class="headerlink" href="#candle-shared-installation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="terminology-and-scope">
<h2>Terminology and scope<a class="headerlink" href="#terminology-and-scope" title="Permalink to this headline">¶</a></h2>
<p>In addition to the CANDLE shared installation being a ready-to-use, central installation of CANDLE, it is further a set of scripts that adds to the functionality of CANDLE and makes it easier to use for new users. For brevity, below these scripts will be called the “wrapper scripts” or “wrappers,” as they are essentially wrappers around the Supervisor/Benchmarks codebase with the aim of improving functionality while leaving the codebase as untouched as possible. These scripts should not interfere in any way with how CANDLE is currently being run; these are only enhancements. They are currently set up and tested on Biowulf and Summit.</p>
<p>The source code of the wrapper scripts is currently located <a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers">here</a>. This contains code that (1) helps to set up and test these
scripts alongside new clones of the
<a class="reference external" href="https://github.com/ECP-CANDLE/Supervisor/tree/develop">Supervisor</a>
and
<a class="reference external" href="https://github.com/ECP-CANDLE/Benchmarks/tree/develop">Benchmarks</a>
repositories, and (2) adds various features to CANDLE. The documentation
for setup (#1) can be found <a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/README.md">here</a>; the documentation
for usage (#2) is below.</p>
</div>
<div class="section" id="overview-of-wrapper-scripts-functionality">
<h2>Overview of wrapper scripts functionality<a class="headerlink" href="#overview-of-wrapper-scripts-functionality" title="Permalink to this headline">¶</a></h2>
<div class="section" id="for-users">
<h3>For users<a class="headerlink" href="#for-users" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p><strong>Run CANDLE as a central installation</strong>. E.g., instead of cloning
the Supervisor and Benchmarks repos as usual and then running a
Supervisor workflow directly in the
<code class="docutils literal notranslate"><span class="pre">Supervisor/workflows/&lt;WORKFLOW&gt;/test</span></code> directory, you would go to
any arbitrary directory on the filesystem, create or edit a single
text file (“input file”), and call CANDLE with the input file as an
argument. This is similar to how other large HPC-enabled software
packages are run, e.g., software for calculating electronic structure</p></li>
<li><p><strong>Edit only a single text input file</strong> to modify <em>everything</em> you
would need to set in order to run a job, e.g., workflow type,
hyperparameter space, number of workers, walltime, “default model”
settings, etc.</p></li>
<li><p><strong>Minimally modify a bare model script</strong>, e.g., no need to add
<code class="docutils literal notranslate"><span class="pre">initialize_parameters()</span></code> and <code class="docutils literal notranslate"><span class="pre">run()</span></code> functions (whose content
occassionally changes) to a new model that you’d like to run using
CANDLE. The wrapper scripts still work for canonically
CANDLE-compliant (i.e., “candelized”) model scripts such as the already-written main
<code class="docutils literal notranslate"><span class="pre">.py</span></code> files used to run the benchmarks. Additional benefits of only
minimally modifying a bare model script:</p>
<ul class="simple">
<li><p>The output of the model using each hyperparameter set is put in
its own file, <code class="docutils literal notranslate"><span class="pre">subprocess_out_and_err.txt</span></code></p></li>
<li><p>Custom environments can be automatically defined for running the
model script using e.g. the keywords <code class="docutils literal notranslate"><span class="pre">supp_modules</span></code>,
<code class="docutils literal notranslate"><span class="pre">python_bin_path</span></code>, <code class="docutils literal notranslate"><span class="pre">exec_python_module</span></code>, <code class="docutils literal notranslate"><span class="pre">supp_pythonpath</span></code>,
described further in the <a class="reference external" href="#input-file-format">section on input file
keywords</a> below for keywords noted to apply
for “minimal CANDLE-compliance only”</p></li>
</ul>
</li>
<li><p><strong>Run model scripts written in other languages</strong> such as <code class="docutils literal notranslate"><span class="pre">R</span></code> and
<code class="docutils literal notranslate"><span class="pre">bash</span></code> (tested on Biowulf but not yet tested on Summit); minimal
additions to the wrapper scripts are needed for adding additional
language support</p></li>
<li><p><strong>Perform a consistent workflow for testing and production jobs</strong>,
i.e.:</p>
<ol class="arabic simple">
<li><p><em>Testing:</em> Using <code class="docutils literal notranslate"><span class="pre">candle</span> <span class="pre">submit-job</span> <span class="pre">&lt;INPUT-FILE&gt;</span></code> with the input
file keyword setting of <code class="docutils literal notranslate"><span class="pre">run_workflow=0</span></code> on an interactive node
(e.g.,
<code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">-W</span> <span class="pre">1:00</span> <span class="pre">-nnodes</span> <span class="pre">1</span> <span class="pre">-P</span> <span class="pre">med106</span> <span class="pre">-q</span> <span class="pre">debug</span> <span class="pre">-Is</span> <span class="pre">/bin/bash</span></code>) for
testing modifications to a model script</p></li>
</ol>
<ul class="simple">
<li><p><strong>Note:</strong> See <a class="reference external" href="#known-issues">here</a> if you encounter an issue with the CUDA driver when testing a model in this interactive mode</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><em>Production:</em> Using <code class="docutils literal notranslate"><span class="pre">candle</span> <span class="pre">submit-job</span> <span class="pre">&lt;INPUT-FILE&gt;</span></code> this time
with the default keyword setting of <code class="docutils literal notranslate"><span class="pre">run_workflow=1</span></code> on a login
node for submitting a CANDLE job as usual</p></li>
</ol>
<ul class="simple">
<li><p>As long as the wrapper scripts are set up properly and your model
script runs successfully using <code class="docutils literal notranslate"><span class="pre">run_workflow=0</span></code>, you can be
pretty confident that submitting the job using <code class="docutils literal notranslate"><span class="pre">run_workflow=1</span></code>
will pick up and run without dying</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="for-developers">
<h3>For developers<a class="headerlink" href="#for-developers" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><strong>Modify only a single file whenever the CANDLE-compliance procedure changes</strong>
(<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/commands/submit-job/candle_compliant_wrapper.py">candle_compliant_wrapper.py</a>). E.g., if the
benchmarks used the minimal modification to the main <code class="docutils literal notranslate"><span class="pre">.py</span></code> files
rather than the traditional CANDLE-compliance procedure, there would
be no need to update every benchmark whenever the CANDLE-compliance
procedure changed</p></li>
<li><p><strong>Edit only a single file in order to make system-specific changes</strong>
(<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/commands/submit-job/preprocess.py">preprocess.py</a>)
such as custom
modification to the <code class="docutils literal notranslate"><span class="pre">$TURBINE_LAUNCH_OPTIONS</span></code> variable; no need to
edit each Supervisor workflow’s <code class="docutils literal notranslate"><span class="pre">workflow.sh</span></code> file</p></li>
</ul>
</div>
</div>
<div class="section" id="loading-the-candle-module">
<h2>Loading the <code class="docutils literal notranslate"><span class="pre">candle</span></code> module<a class="headerlink" href="#loading-the-candle-module" title="Permalink to this headline">¶</a></h2>
<p>We are currently getting CANDLE approved as user-managed software on
Summit. Once it is approved, we will be able to load the <code class="docutils literal notranslate"><span class="pre">candle</span></code>
module via <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">candle</span></code>. In the interim, do this instead:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span> /gpfs/alpine/med106/world-shared/candle/env_for_lmod-tf2.sh
</pre></div>
</div>
<p>Both methods primarily do the following:</p>
<ul class="simple">
<li><p>Sets the <code class="docutils literal notranslate"><span class="pre">$CANDLE</span></code> variable to
<code class="docutils literal notranslate"><span class="pre">/gpfs/alpine/med106/world-shared/candle/tf2</span></code> in order to set the
top-level directory of the entire CANDLE file tree, including the
<code class="docutils literal notranslate"><span class="pre">Supervisor</span></code>, <code class="docutils literal notranslate"><span class="pre">Benchmarks</span></code>, and <code class="docutils literal notranslate"><span class="pre">wrappers</span></code> GitHub repositories</p></li>
<li><p>Appends <code class="docutils literal notranslate"><span class="pre">$CANDLE/wrappers/bin</span></code> to <code class="docutils literal notranslate"><span class="pre">$PATH</span></code> in order to be able to
run <code class="docutils literal notranslate"><span class="pre">candle</span></code> from the command line</p></li>
<li><p>Sets the <code class="docutils literal notranslate"><span class="pre">$SITE</span></code> variable to <code class="docutils literal notranslate"><span class="pre">summit-tf2</span></code> in order to specify the
HPC system and environment</p></li>
<li><p>Appends <code class="docutils literal notranslate"><span class="pre">$CANDLE/Benchmarks/common</span></code> to <code class="docutils literal notranslate"><span class="pre">$PYTHONPATH</span></code> to allow one
to write a Python model script in an arbitrary directory and to be
able to run <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">candle</span></code> in the script</p></li>
</ul>
</div>
<div class="section" id="quick-start-examples-for-summit">
<h2>Quick-start examples (for Summit)<a class="headerlink" href="#quick-start-examples-for-summit" title="Permalink to this headline">¶</a></h2>
<div class="section" id="step-1-setup">
<h3>Step 1: Setup<a class="headerlink" href="#step-1-setup" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the CANDLE module; do the following for the time being in lieu of &quot;module load candle&quot;, as we are currently getting CANDLE approved as user-managed software</span>
<span class="nb">source</span> /gpfs/alpine/med106/world-shared/candle/env_for_lmod-tf2.sh

<span class="c1"># Enter a possibly empty directory that is completely outside of the Supervisor/Benchmarks repositories on the Alpine filesystem, such as $MEMBERWORK</span>
<span class="nb">cd</span> /gpfs/alpine/med106/scratch/weismana/notebook/2020-11-13/testing_candle_installation
</pre></div>
</div>
</div>
<div class="section" id="step-2-run-sample-candle-compliant-model-scripts">
<h3>Step 2: Run sample CANDLE-compliant model scripts<a class="headerlink" href="#step-2-run-sample-candle-compliant-model-scripts" title="Permalink to this headline">¶</a></h3>
<p>This refers to model scripts that the developers refer to as
“CANDLE-compliant” or “candelized” as usual.</p>
<div class="section" id="nt3-using-upf-candle-compliant-model-scripts">
<h4>NT3 using UPF (CANDLE-compliant model scripts)<a class="headerlink" href="#nt3-using-upf-candle-compliant-model-scripts" title="Permalink to this headline">¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the UPF example (one file will be copied over)</span>
candle import-template upf

<span class="c1"># Submit the job to the queue</span>
candle submit-job upf_example.in
</pre></div>
</div>
</div>
<div class="section" id="nt3-using-mlrmbo-candle-compliant-model-scripts">
<h4>NT3 using mlrMBO (CANDLE-compliant model scripts)<a class="headerlink" href="#nt3-using-mlrmbo-candle-compliant-model-scripts" title="Permalink to this headline">¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the mlrMBO example (two files will be copied over)</span>
candle import-template mlrmbo

<span class="c1"># Submit the job to the queue</span>
candle submit-job mlrmbo_example.in
</pre></div>
</div>
</div>
</div>
<div class="section" id="step-3-run-sample-non-candle-compliant-model-scripts">
<h3>Step 3: Run sample <strong>non</strong>-CANDLE-compliant model scripts<a class="headerlink" href="#step-3-run-sample-non-candle-compliant-model-scripts" title="Permalink to this headline">¶</a></h3>
<p>This refers to model scripts that have gone from “bare” (e.g., one
downloaded directly from the Internet) to “minimally modified,” a
process described
<a class="reference external" href="#how-to-minimally-modify-a-bare-model-script-for-use-with-the-wrapper-scripts">below</a>.</p>
<div class="section" id="mnist-using-upf-non-candle-compliant-model-scripts">
<h4>MNIST using UPF (non-CANDLE-compliant model scripts)<a class="headerlink" href="#mnist-using-upf-non-candle-compliant-model-scripts" title="Permalink to this headline">¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pre-fetch the MNIST data since Summit compute nodes can&#39;t access the Internet (this has nothing to do with the wrapper scripts)</span>
mkdir candle_generated_files
/gpfs/alpine/world-shared/med106/sw/condaenv-200408/bin/python -c <span class="s2">&quot;from keras.datasets import mnist; import os; (x_train, y_train), (x_test, y_test) = mnist.load_data(os.path.join(os.getcwd(), &#39;candle_generated_files&#39;, &#39;mnist.npz&#39;))&quot;</span>

<span class="c1"># Import the grid example (two files will be copied over)</span>
candle import-template grid

<span class="c1"># Submit the job to the queue</span>
candle submit-job grid_example.in
</pre></div>
</div>
</div>
<div class="section" id="nt3-using-mlrmbo-non-candle-compliant-model-scripts">
<h4>NT3 using mlrMBO (non-CANDLE-compliant model scripts)<a class="headerlink" href="#nt3-using-mlrmbo-non-candle-compliant-model-scripts" title="Permalink to this headline">¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the bayesian example (two files will be copied over)</span>
candle import-template bayesian

<span class="c1"># Submit the job to the queue</span>
candle submit-job bayesian_example.in
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="how-to-minimally-modify-a-bare-model-script-for-use-with-the-wrapper-scripts">
<h2>How to minimally modify a bare model script for use with the wrapper scripts<a class="headerlink" href="#how-to-minimally-modify-a-bare-model-script-for-use-with-the-wrapper-scripts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Set the hyperparameters in the model script using a dictionary called
<code class="docutils literal notranslate"><span class="pre">candle_params</span></code></p></li>
<li><p>Ensure somewhere near the end of the script either the normal
<code class="docutils literal notranslate"><span class="pre">history</span></code> object is defined or a metric of how well the
hyperparameter set performed (a value you want to minimize, such as
the loss evaluated on a test set) is returned as a number in the
<code class="docutils literal notranslate"><span class="pre">candle_value_to_return</span></code> variable</p></li>
</ol>
<p>This is demonstrated in
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/examples/summit-tf2/grid/mnist_mlp.py">$CANDLE/wrappers/examples/summit-tf2/grid/mnist_mlp.py</a>.</p>
<div class="section" id="running-a-non-candle-compliant-model-on-its-own-outside-of-supervisor">
<h3>Running a non-CANDLE-compliant model on its own, outside of Supervisor<a class="headerlink" href="#running-a-non-candle-compliant-model-on-its-own-outside-of-supervisor" title="Permalink to this headline">¶</a></h3>
<p>One drawback to minimally modifying a bare model script as opposed to
making it fully CANDLE-compliant is that the former cannot generally run
standalone (which you should only do on an interactive node), e.g.,
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">my_model_script.py</span></code>. There are two simple ways to handle this:</p>
<ol class="arabic simple">
<li><p>Use the recommended workflow of setting <code class="docutils literal notranslate"><span class="pre">run_workflow=0</span></code> and then
running the model script using <code class="docutils literal notranslate"><span class="pre">candle</span> <span class="pre">submit-job</span> <span class="pre">my_input_file.in</span></code></p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">bash</span> <span class="pre">run_candle_model_standalone.sh</span></code>. Explanation: The first
time a minimally CANDLE-compliant model script is run, using either
setting of <code class="docutils literal notranslate"><span class="pre">run_workflow</span></code>, a file called
<code class="docutils literal notranslate"><span class="pre">run_candle_model_standalone.sh</span></code> is created, which runs
<code class="docutils literal notranslate"><span class="pre">candle_compliant_wrapper.py</span></code> using Python, just as you’re desiring
to run a fully CANDLE-compliant model script using Python in this
situation. (As some environment variables are required to be set in
<code class="docutils literal notranslate"><span class="pre">candle_compliant_wrapper.py</span></code> and the files it calls,
<code class="docutils literal notranslate"><span class="pre">run_candle_model_standalone.sh</span></code> also sets some environment
variables.)</p></li>
</ol>
<p>Aside from not needing to make a model script fully CANDLE-compliant,
the usual advantages of running minimally CANDLE-compliant scripts like
this apply here, e.g., model scripts can be written in other languages
and a custom environment can be automatically defined via, e.g.,
<code class="docutils literal notranslate"><span class="pre">supp_modules</span></code>, <code class="docutils literal notranslate"><span class="pre">python_bin_path</span></code>, <code class="docutils literal notranslate"><span class="pre">exec_python_module</span></code>,
<code class="docutils literal notranslate"><span class="pre">supp_pythonpath</span></code>.</p>
<p>As usual for miminally CANDLE-compliant model scripts, the output of the
script is placed in <code class="docutils literal notranslate"><span class="pre">subprocess_out_and_err.txt</span></code>.</p>
</div>
</div>
<div class="section" id="input-file-format">
<h2>Input file format<a class="headerlink" href="#input-file-format" title="Permalink to this headline">¶</a></h2>
<p>The input file should contain three sections: <code class="docutils literal notranslate"><span class="pre">&amp;control</span></code>,
<code class="docutils literal notranslate"><span class="pre">&amp;default_model</span></code>, and <code class="docutils literal notranslate"><span class="pre">&amp;param_space</span></code>. Each section should start with
this header on its own line and end with <code class="docutils literal notranslate"><span class="pre">/</span></code> on its own line. (This
input file format is based on the <a class="reference external" href="https://www.quantum-espresso.org/">Quantum
Espresso</a> electronic structure
software.) Four sample input files, corresponding to the four examples
in the <a class="reference external" href="#quick-start-examples-for-summit">quick-start examples
above</a>, are here:
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/examples/summit-tf2/upf/upf_example.in">upf</a>,
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/examples/summit-tf2/mlrmbo/mlrmbo_example.in">mlrmbo</a>,
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/examples/summit-tf2/grid/grid_example.in">grid</a>,
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/examples/summit-tf2/bayesian/bayesian_example.in">bayesian</a>.
Spaces at the beginnings of the content-containing lines are optional
but are recommended for readability.</p>
<div class="section" id="control-section">
<h3><code class="docutils literal notranslate"><span class="pre">&amp;control</span></code> section<a class="headerlink" href="#control-section" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">&amp;control</span></code> section contains all settings aside from those
specified in the <code class="docutils literal notranslate"><span class="pre">&amp;default_model</span></code> and <code class="docutils literal notranslate"><span class="pre">&amp;param_space</span></code> sections
(detailed below) in the format <code class="docutils literal notranslate"><span class="pre">keyword</span> <span class="pre">=</span> <span class="pre">value</span></code>. Spaces around the
<code class="docutils literal notranslate"><span class="pre">=</span></code> sign are optional, and each keyword setting should be on its own
line. Each <code class="docutils literal notranslate"><span class="pre">value</span></code> ultimately gets interpreted by <code class="docutils literal notranslate"><span class="pre">bash</span></code> and hence
is taken to be a string by default; thus, quotes are not necessary for
string <code class="docutils literal notranslate"><span class="pre">value</span></code>s.</p>
<p>Here is a list of possible <code class="docutils literal notranslate"><span class="pre">keyword</span></code>s and their default <code class="docutils literal notranslate"><span class="pre">value</span></code>s
(if <code class="docutils literal notranslate"><span class="pre">None</span></code>, then the keyword is required), as specified in
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/site-specific_settings.sh">$CANDLE/wrappers/site-specific_settings.sh</a>:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><code class="docutils literal notranslate"><span class="pre">keyword</span></code></p></th>
<th class="head"><p>Default <code class="docutils literal notranslate"><span class="pre">value</span></code></p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">model_script</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Full path to the
model script</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">workflow</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>Currently only
<code class="docutils literal notranslate"><span class="pre">grid</span></code> and
<code class="docutils literal notranslate"><span class="pre">bayesian</span></code> are
enabled (which get
mapped to the UPF and
mlrMBO Supervisor
workflows)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">project</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code></p></td>
<td><p>OLCF project to use,
e.g., <code class="docutils literal notranslate"><span class="pre">med106</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">walltime</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">00:05</span></code></p></td>
<td><p>In <code class="docutils literal notranslate"><span class="pre">HH:MM</span></code> format
as is used on Summit</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">nworkers</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1</span></code></p></td>
<td><p>workers=GPUs. The
number of nodes used
on Summit
will be
ceil((<code class="docutils literal notranslate"><span class="pre">nworkers</span></code>
+ (1 (<code class="docutils literal notranslate"><span class="pre">grid</span></code>) or 2
(<code class="docutils literal notranslate"><span class="pre">bayesian</span></code>))) /
6),
after which 0-5
workers will be added
in order to utilize
all GPUs on the nodes</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">dl_backend</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">keras</span></code></p></td>
<td><p>Valid backends are
<code class="docutils literal notranslate"><span class="pre">keras</span></code> and
<code class="docutils literal notranslate"><span class="pre">pytorch</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">supp_modules</span></code></p></td>
<td><p>Empty string</p></td>
<td><p>Supplementary
<code class="docutils literal notranslate"><span class="pre">module</span></code>s to load
prior to executing a
model script (minimal
CANDLE-compliance
only)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">python_bin_path</span></code></p></td>
<td><p>Empty string</p></td>
<td><p>Actual Python version
to use if not the one
set in
<code class="docutils literal notranslate"><span class="pre">env-$SITE.sh</span></code>
(minimal
CANDLE-compliance
only)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">exec_python_module</span></code></p></td>
<td><p>Empty string</p></td>
<td><p>Actual Python
<code class="docutils literal notranslate"><span class="pre">module</span></code> to use if
not the Python
version set in
<code class="docutils literal notranslate"><span class="pre">env-$SITE.sh</span></code>
(minimal
CANDLE-compliance
only)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">supp_pythonpath</span></code></p></td>
<td><p>Empty string</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">:</span></code>-delimited list
of <code class="docutils literal notranslate"><span class="pre">$PYTHONPATH</span></code>
settings to append to
the <code class="docutils literal notranslate"><span class="pre">$PYTHONPATH</span></code>
variable (minimal
CANDLE-compliance
only)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">extra_script_args</span></code></p></td>
<td><p>Empty string</p></td>
<td><p>Extra arguments to
the <code class="docutils literal notranslate"><span class="pre">python</span></code> or
<code class="docutils literal notranslate"><span class="pre">R</span></code> programs to use
when calling the
corresponding model
script (minimal
CANDLE-compliance
only)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">exec_r_module</span></code></p></td>
<td><p>Empty string</p></td>
<td><p>Actual R <code class="docutils literal notranslate"><span class="pre">module</span></code>
to use if not the R
version set in
<code class="docutils literal notranslate"><span class="pre">env-$SITE.sh</span></code>
(minimal
CANDLE-compliance
only)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">supp_r_libs</span></code></p></td>
<td><p>Empty string</p></td>
<td><p>Full path to a
supplementary
<code class="docutils literal notranslate"><span class="pre">$R_LIBS</span></code> library
to use (minimal
CANDLE-compliance
only)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">run_workflow</span></code></p></td>
<td><p>1</p></td>
<td><p>0 will run your model
script once using the
default model
parameters on the
current node (so only
use this on an
interactive node); 1
will run the actual
Supervisor workflow,
submitting the job to
the queue as usual</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">dry_run</span></code></p></td>
<td><p>0</p></td>
<td><p>1 will set up the job
but not execute it so
that you can examine
the settings files
generated in the
submission directory;
0 will run the job as
usual</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">queue</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">batch</span></code></p></td>
<td><p>Partition to use for
the CANDLE job</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">design_size</span></code></p></td>
<td><p><a class="reference external" href="#contribution-ideas">Not yet
preprocessed</a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bayesian</span></code> workflow
only; total number of
points to sample
within the
hyperparameter space
prior to running the
<a class="reference external" href="https://cran.r-project.org/web/packages/mlrMBO/vignettes/mlrMBO.html">mlrMBO
algorithm</a>.
E.g.,
<code class="docutils literal notranslate"><span class="pre">design_size</span> <span class="pre">=</span> <span class="pre">9</span></code>.
Note that this must
be greater than or
equal to the largest
number of possible
values for any
discrete
hyperparameter
specified in the
<code class="docutils literal notranslate"><span class="pre">&amp;param_space</span></code>
section. A reasonable
value for this (and
for
<code class="docutils literal notranslate"><span class="pre">propose_points</span></code>,
below) is 15-20</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">propose_points</span></code></p></td>
<td><p><a class="reference external" href="#contribution-ideas">Not yet
preprocessed</a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bayesian</span></code> workflow
only; number of
proposed (really
evaluated) points at
each <a class="reference external" href="https://www.rdocumentation.org/packages/mlrMBO/versions/1.1.2/topics/makeMBOControl">MBO
iteration</a>.
E.g.,
<code class="docutils literal notranslate"><span class="pre">propose_points</span> <span class="pre">=</span> <span class="pre">9</span></code>
. A reasonable value
for this (and for
<code class="docutils literal notranslate"><span class="pre">design_size</span></code>,
above) is 15-20</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">max_iterations</span></code></p></td>
<td><p><a class="reference external" href="#contribution-ideas">Not yet
preprocessed</a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bayesian</span></code> workflow
only; maximum number
of <a class="reference external" href="https://www.rdocumentation.org/packages/mlrMBO/versions/1.1.2/topics/setMBOControlTermination">sequential
optimization
steps</a>.
E.g.,
<code class="docutils literal notranslate"><span class="pre">max_iterations</span> <span class="pre">=</span> <span class="pre">3</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">max_budget</span></code></p></td>
<td><p><a class="reference external" href="#contribution-ideas">Not yet
preprocessed</a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bayesian</span></code> workflow
only; maximum total
number of <a class="reference external" href="https://www.rdocumentation.org/packages/mlrMBO/versions/1.1.2/topics/setMBOControlTermination">function
evaluations</a>
for all iterations
combined. E.g.,
<code class="docutils literal notranslate"><span class="pre">max_budget</span> <span class="pre">=</span> <span class="pre">180</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="default-model-section">
<h3><code class="docutils literal notranslate"><span class="pre">&amp;default_model</span></code> section<a class="headerlink" href="#default-model-section" title="Permalink to this headline">¶</a></h3>
<p>This can contain either a single keyword/value line containing the
<code class="docutils literal notranslate"><span class="pre">candle_default_model_file</span></code> keyword pointing to the full path of the
default model text file to use, e.g.,
<code class="docutils literal notranslate"><span class="pre">candle_default_model_file</span> <span class="pre">=</span> <span class="pre">$CANDLE/Benchmarks/Pilot1/NT3/nt3_default_model.txt</span></code>
or the <em>contents</em> of such a default model file as, e.g., in the
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/examples/summit-tf2/grid/grid_example.in">grid</a>
or
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/examples/summit-tf2/bayesian/bayesian_example.in">bayesian</a>
examples in the <a class="reference external" href="#quick-start-examples-for-summit">quick-start section
above</a>.</p>
</div>
<div class="section" id="param-space-section">
<h3><code class="docutils literal notranslate"><span class="pre">&amp;param_space</span></code> section<a class="headerlink" href="#param-space-section" title="Permalink to this headline">¶</a></h3>
<p>This can contain either a single keyword/value line containing the
<code class="docutils literal notranslate"><span class="pre">candle_param_space_file</span></code> keyword pointing to the full path of the
file specifying the hyperparameter space to use, e.g.,
<code class="docutils literal notranslate"><span class="pre">candle_param_space_file</span> <span class="pre">=</span> <span class="pre">$CANDLE/Supervisor/workflows/mlrMBO/data/nt3_nightly.R</span></code>
or the <em>contents</em> of such a parameter space file as, e.g., in the
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/examples/summit-tf2/grid/grid_example.in">grid</a>
or
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/examples/summit-tf2/upf/upf_example.in">upf</a>
examples in the <a class="reference external" href="#quick-start-examples-for-summit">quick-start section
above</a> or here:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&amp;param_space
  makeDiscreteParam(&quot;batch_size&quot;, values = c(16, 32))
  makeIntegerParam(&quot;epochs&quot;, lower = 2, upper = 5)
  makeDiscreteParam(&quot;optimizer&quot;, values = c(&quot;adam&quot;, &quot;sgd&quot;, &quot;rmsprop&quot;, &quot;adagrad&quot;, &quot;adadelta&quot;))
  makeNumericParam(&quot;dropout&quot;, lower = 0, upper = 0.9)
  makeNumericParam(&quot;learning_rate&quot;, lower = 0.00001, upper = 0.1)
/
</pre></div>
</div>
<p>Note there are no commas at the end of each line in the example above.</p>
</div>
</div>
<div class="section" id="code-organization">
<h2>Code organization<a class="headerlink" href="#code-organization" title="Permalink to this headline">¶</a></h2>
<p>A description of what every file does in the <a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers">wrappers
repository</a>, which
is cloned to <code class="docutils literal notranslate"><span class="pre">$CANDLE/wrappers</span></code>, can be found
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/repository_organization.md">here</a>. Some particular notes:</p>
<ul class="simple">
<li><p>In addition to the page you are reading, all documentation is currently in the top-level directory:
<code class="docutils literal notranslate"><span class="pre">README.md</span></code> (see this file for additional notes),
<code class="docutils literal notranslate"><span class="pre">repository_organization.md</span></code>, <code class="docutils literal notranslate"><span class="pre">setup-biowulf.md</span></code>, and
<code class="docutils literal notranslate"><span class="pre">setup-summit.md</span></code></p></li>
<li><p>Directories pertaining to the <strong>setup</strong> of the wrappers repository
and in general of CANDLE on a new HPC system (involved in the <a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/README.md">setup
documentation</a>) are <code class="docutils literal notranslate"><span class="pre">log_files</span></code>, <code class="docutils literal notranslate"><span class="pre">swift-t_setup</span></code>,
and <code class="docutils literal notranslate"><span class="pre">test_files</span></code></p></li>
<li><p>Directories pertaining to the <strong>usage</strong> of the wrapper scripts
(involved in the usage documentation that you are currently reading)
are:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">lmod_modules</span></code>: contains <code class="docutils literal notranslate"><span class="pre">.lua</span></code> files used by the <code class="docutils literal notranslate"><span class="pre">lmod</span></code>
system for loading <code class="docutils literal notranslate"><span class="pre">module</span></code>s, enabling one to run, e.g.,
<a class="reference external" href="#loading-the-candle-module">module load candle</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bin</span></code>: contains a single script called <code class="docutils literal notranslate"><span class="pre">candle</span></code> that can be
accessed by typing <code class="docutils literal notranslate"><span class="pre">candle</span></code> on the command line once the CANDLE
module has been loaded. You can generate a usage message by simply
typing <code class="docutils literal notranslate"><span class="pre">candle</span></code> or <code class="docutils literal notranslate"><span class="pre">candle</span> <span class="pre">help</span></code> on the command line and
hitting Enter</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">examples</span></code>: contains sample/template input files and model
scripts for different <code class="docutils literal notranslate"><span class="pre">$SITE</span></code>s</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">commands</span></code>: contains one directory so-named for each command to
the <code class="docutils literal notranslate"><span class="pre">candle</span></code> program, each containing all files related to the
command. Possible commands are <code class="docutils literal notranslate"><span class="pre">import-template</span></code>, <code class="docutils literal notranslate"><span class="pre">generate-grid</span></code>, <code class="docutils literal notranslate"><span class="pre">submit-job</span></code>, and <code class="docutils literal notranslate"><span class="pre">aggregate-results</span></code>. The file called <code class="docutils literal notranslate"><span class="pre">command_script.sh</span></code> in each command’s
directory is the main file called when the command is run using
<code class="docutils literal notranslate"><span class="pre">candle</span> <span class="pre">&lt;COMMAND&gt;</span> <span class="pre">...</span></code>. The only command not currently tested on
Summit is <code class="docutils literal notranslate"><span class="pre">aggregate-results</span></code>. The bulk of the files involved in
the functionality described in this document correspond to the
<code class="docutils literal notranslate"><span class="pre">submit-job</span></code> command, i.e., are located in the <code class="docutils literal notranslate"><span class="pre">submit-job</span></code>
subdirectory</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="recommendations-for-particular-use-cases">
<h2>Recommendations for particular use cases<a class="headerlink" href="#recommendations-for-particular-use-cases" title="Permalink to this headline">¶</a></h2>
<div class="section" id="run-grid-or-bayesian-hyperparameter-searches-on-an-already-candle-compliant-model-script-such-as-a-benchmark">
<h3>Run <code class="docutils literal notranslate"><span class="pre">grid</span></code> or <code class="docutils literal notranslate"><span class="pre">bayesian</span></code> hyperparameter searches on an already CANDLE-compliant model script such as a benchmark<a class="headerlink" href="#run-grid-or-bayesian-hyperparameter-searches-on-an-already-candle-compliant-model-script-such-as-a-benchmark" title="Permalink to this headline">¶</a></h3>
<p>Note that you can copy a benchmark to your working directory and make
the modifications there, as the templates show.</p>
<ol class="arabic simple">
<li><p>Enter a directory on Summit’s Alpine filesystem such as
<code class="docutils literal notranslate"><span class="pre">$MEMBERWORK</span></code></p></li>
<li><p>Load the <code class="docutils literal notranslate"><span class="pre">candle</span></code> module via
<code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">/gpfs/alpine/med106/world-shared/candle/env_for_lmod-tf2.sh</span></code></p></li>
<li><p>Import one of the <a class="reference external" href="#step-2-run-sample-candle-compliant-model-scripts">templates for running canonically CANDLE-compliant
models</a> using
<code class="docutils literal notranslate"><span class="pre">candle</span> <span class="pre">import-template</span> <span class="pre">{upf|mlrmbo}</span></code> and delete all but the copied-over input
file</p></li>
<li><p>Rename and tweak the input file to your liking using the
<a class="reference external" href="#input-file-format">documentation for input files</a> above</p></li>
<li><p>Ensure your model runs on an interactive node (e.g.,
<code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">-W</span> <span class="pre">1:00</span> <span class="pre">-nnodes</span> <span class="pre">1</span> <span class="pre">-P</span> <span class="pre">med106</span> <span class="pre">-q</span> <span class="pre">debug</span> <span class="pre">-Is</span> <span class="pre">/bin/bash</span></code>) by
setting the <code class="docutils literal notranslate"><span class="pre">run_workflow=0</span></code> keyword setting in the <code class="docutils literal notranslate"><span class="pre">&amp;control</span></code>
section of the input file and running
<code class="docutils literal notranslate"><span class="pre">candle</span> <span class="pre">submit-job</span> <span class="pre">&lt;INPUT-FILE&gt;</span></code></p></li>
<li><p>Submit your job from a login node by setting the default setting of
<code class="docutils literal notranslate"><span class="pre">run_workflow=1</span></code> in the <code class="docutils literal notranslate"><span class="pre">&amp;control</span></code> section of the input file and
running <code class="docutils literal notranslate"><span class="pre">candle</span> <span class="pre">submit-job</span> <span class="pre">&lt;INPUT-FILE&gt;</span></code></p></li>
</ol>
</div>
<div class="section" id="create-a-new-model-script-on-which-you-want-to-run-grid-or-bayesian-hyperparameter-searches">
<h3>Create a new model script on which you want to run <code class="docutils literal notranslate"><span class="pre">grid</span></code> or <code class="docutils literal notranslate"><span class="pre">bayesian</span></code> hyperparameter searches<a class="headerlink" href="#create-a-new-model-script-on-which-you-want-to-run-grid-or-bayesian-hyperparameter-searches" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p>Enter a directory on Summit’s Alpine filesystem such as
<code class="docutils literal notranslate"><span class="pre">$MEMBERWORK</span></code></p></li>
<li><p>Load the <code class="docutils literal notranslate"><span class="pre">candle</span></code> module via
<code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">/gpfs/alpine/med106/world-shared/candle/env_for_lmod-tf2.sh</span></code></p></li>
<li><p>Create a bare model script as usual (e.g., download a model from the
Internet, tweak it, and apply it on your data)</p></li>
<li><p>Make the model script <em>minimally</em> CANDLE-compliant as described
<a class="reference external" href="#how-to-minimally-modify-a-bare-model-script-for-use-with-the-wrapper-scripts">above</a></p></li>
<li><p>Import one of the <a class="reference external" href="#step-3-run-sample-non-candle-compliant-model-scripts">templates for running minimally CANDLE-compliant
models</a>
using <code class="docutils literal notranslate"><span class="pre">candle</span> <span class="pre">import-template</span> <span class="pre">{grid|bayesian}</span></code>; delete all but the
input file</p></li>
<li><p>Rename and tweak the input file to your liking using the
<a class="reference external" href="#input-file-format">documentation for input files</a> above</p></li>
<li><p>Ensure your model runs on an interactive node (e.g.,
<code class="docutils literal notranslate"><span class="pre">bsub</span> <span class="pre">-W</span> <span class="pre">1:00</span> <span class="pre">-nnodes</span> <span class="pre">1</span> <span class="pre">-P</span> <span class="pre">med106</span> <span class="pre">-q</span> <span class="pre">debug</span> <span class="pre">-Is</span> <span class="pre">/bin/bash</span></code>) by
setting the <code class="docutils literal notranslate"><span class="pre">run_workflow=0</span></code> keyword setting in the <code class="docutils literal notranslate"><span class="pre">&amp;control</span></code>
section of the input file and running
<code class="docutils literal notranslate"><span class="pre">candle</span> <span class="pre">submit-job</span> <span class="pre">&lt;INPUT-FILE&gt;</span></code></p></li>
<li><p>Submit your job from a login node by setting the default setting of
<code class="docutils literal notranslate"><span class="pre">run_workflow=1</span></code> in the <code class="docutils literal notranslate"><span class="pre">&amp;control</span></code> section of the input file and
running <code class="docutils literal notranslate"><span class="pre">candle</span> <span class="pre">submit-job</span> <span class="pre">&lt;INPUT-FILE&gt;</span></code></p></li>
</ol>
</div>
<div class="section" id="run-a-model-script-written-in-another-language-such-as-r-or-bash">
<h3>Run a model script written in another language such as <code class="docutils literal notranslate"><span class="pre">R</span></code> or <code class="docutils literal notranslate"><span class="pre">bash</span></code><a class="headerlink" href="#run-a-model-script-written-in-another-language-such-as-r-or-bash" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="#how-to-contact-andrew-for-help-with-anything-above">Ask Andrew Weisman</a> to
test this first because he hasn’t tested it on Summit yet.</p>
</div>
<div class="section" id="pull-updates-to-the-central-installation-of-candle-that-have-already-been-pulled-into-the-main-supervisor-benchmarks-repositories">
<h3>Pull updates to the central installation of CANDLE that have already been pulled into the main Supervisor/Benchmarks repositories<a class="headerlink" href="#pull-updates-to-the-central-installation-of-candle-that-have-already-been-pulled-into-the-main-supervisor-benchmarks-repositories" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p>Load the <code class="docutils literal notranslate"><span class="pre">candle</span></code> module via
<code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">/gpfs/alpine/med106/world-shared/candle/env_for_lmod-tf2.sh</span></code></p></li>
<li><p>Enter the clone you’d like to update via <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">$CANDLE/Supervisor</span></code> or
<code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">$CANDLE/Benchmarks</span></code></p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">pull</span></code>, adjusting the permissions if necessary the very
first time (or <a class="reference external" href="#how-to-contact-andrew-for-help-with-anything-above">ask
Andrew</a> to do
this)</p></li>
</ol>
</div>
<div class="section" id="commit-changes-to-the-wrapper-scripts-or-to-the-supervisor-or-benchmarks-clones-in-the-central-installation">
<h3>Commit changes to the wrapper scripts or to the Supervisor or Benchmarks clones in the central installation<a class="headerlink" href="#commit-changes-to-the-wrapper-scripts-or-to-the-supervisor-or-benchmarks-clones-in-the-central-installation" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p>Load the <code class="docutils literal notranslate"><span class="pre">candle</span></code> module via
<code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">/gpfs/alpine/med106/world-shared/candle/env_for_lmod-tf2.sh</span></code></p></li>
<li><p>Enter the clone you’d like to update via
<code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">$CANDLE/{wrappers|Supervisor|Benchmarks}</span></code></p></li>
<li><p>Make your modifications to the code and commit your changes,
adjusting the permissions if necessary the very first time (or <a class="reference external" href="#how-to-contact-andrew-for-help-with-anything-above">ask
Andrew</a> to do
this)</p></li>
<li><p><a class="reference external" href="#how-to-contact-andrew-for-help-with-anything-above">Ask Andrew</a>
to push the changes to newly forked versions of the corresponding
repositories and submit pull requests into the main versions of the
repositories</p></li>
</ol>
</div>
</div>
<div class="section" id="contribution-ideas">
<h2>Contribution ideas<a class="headerlink" href="#contribution-ideas" title="Permalink to this headline">¶</a></h2>
<p>Feel free to make any changes you’d like to the code and commit them via
the <a class="reference external" href="#commit-changes-to-the-wrapper-scripts-or-to-the-supervisor-or-benchmarks-clones-in-the-central-installation">preliminary workflow
above</a>.
Below are some ideas for particular ways to contribute:</p>
<ul class="simple">
<li><p>Implement workflows other than <code class="docutils literal notranslate"><span class="pre">grid</span></code> and <code class="docutils literal notranslate"><span class="pre">bayesian</span></code> (UQ would be
great!) by following the instructions
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/README.md#how-to-add-new-workflows">here</a></p></li>
<li><p>If this is something you personally want, allow for command-line
arguments to the <code class="docutils literal notranslate"><span class="pre">candle</span></code> command, such as <code class="docutils literal notranslate"><span class="pre">run_workflow</span></code> or any
other <a class="reference external" href="#input-file-format">input file keywords</a></p></li>
<li><p>Check/preprocess the four mlrMBO keywords (<code class="docutils literal notranslate"><span class="pre">design_size</span></code>,
<code class="docutils literal notranslate"><span class="pre">propose_points</span></code>, <code class="docutils literal notranslate"><span class="pre">max_iterations</span></code>, <code class="docutils literal notranslate"><span class="pre">max_budget</span></code>) by following
the instructions <a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/README.md#how-to-add-a-new-keyword">here</a> and
seeing their usage
<a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/commands/submit-job/dummy_cfg-prm.sh">here</a>
(good exercise to get familiar with the wrappers code)</p></li>
<li><p>Anything else!</p></li>
</ul>
</div>
<div class="section" id="known-issues">
<h2>Known issues<a class="headerlink" href="#known-issues" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><strong>CUDA driver.</strong> If, when running on an interactive node (using <code class="docutils literal notranslate"><span class="pre">run_workflow=0</span></code> in the input file), you get an error like <code class="docutils literal notranslate"><span class="pre">tensorflow.python.framework.errors_impl.InternalError:</span> <span class="pre">cudaGetDevice()</span> <span class="pre">failed.</span> <span class="pre">Status:</span> <span class="pre">CUDA</span> <span class="pre">driver</span> <span class="pre">version</span> <span class="pre">is</span> <span class="pre">insufficient</span> <span class="pre">for</span> <span class="pre">CUDA</span> <span class="pre">runtime</span> <span class="pre">version</span></code> then likely you need to load the CUDA module corresponding to that which is automatically loaded in batch mode, based on the contents of <code class="docutils literal notranslate"><span class="pre">$CANDLE/Supervisor/workflows/common/sh/env-summit-tf2.sh</span></code>; currently, this means that you need to run <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">cuda/10.2.89</span></code>. Explanation: When following the interactive protocol for testing, only the default version of Python is loaded prior to running the model using the default model settings, as opposed to the CUDA module being loaded as well. Note: This is a relatively new issue.</p></li>
<li><p><strong>InvalidArgumentError.</strong> You may need to add <code class="docutils literal notranslate"><span class="pre">K.clear_session()</span></code> prior to, say, <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Sequential()</span></code> in a Keras-based model. Otherwise, once the same rank runs a model script a <em>second</em> time, we get a strange <code class="docutils literal notranslate"><span class="pre">InvalidArgumentError</span></code> error that kills Supervisor (see the comments in <a class="reference external" href="https://github.com/ECP-CANDLE/Benchmarks/blob/develop/Pilot1/NT3/nt3_candle_wrappers_baseline_keras2.py">$CANDLE/Benchmarks/Pilot1/NT3/nt3_candle_wrappers_baseline_keras2.py</a> for more details). It is wholly possible that this is a bug that has gotten fixed in subsequent versions of Keras/Tensorflow.</p></li>
<li><p><strong>Path to CANDLE library.</strong> If you, say, pull a Benchmark model script out of the <code class="docutils literal notranslate"><span class="pre">Benchmarks</span></code> repository into your own separate directory, you may need to add a line like <code class="docutils literal notranslate"><span class="pre">sys.path.append(os.path.join(os.getenv('CANDLE'),</span> <span class="pre">'Benchmarks',</span> <span class="pre">'Pilot1',</span> <span class="pre">'NT3'))</span></code>. This is demonstrated in <a class="reference external" href="https://github.com/fnlcr-bids-sdsi/candle_wrappers/blob/master/examples/summit-tf2/mlrmbo/nt3_baseline_keras2.py">$CANDLE/wrappers/examples/summit-tf2/mlrmbo/nt3_baseline_keras2.py</a>.</p></li>
</ul>
</div>
<div class="section" id="how-to-contact-andrew-for-help-with-anything-above">
<h2>How to contact Andrew for help with anything above<a class="headerlink" href="#how-to-contact-andrew-for-help-with-anything-above" title="Permalink to this headline">¶</a></h2>
<div class="line-block">
<div class="line">Email: <a class="reference external" href="mailto:andrew&#46;weisman&#37;&#52;&#48;nih&#46;gov">andrew<span>&#46;</span>weisman<span>&#64;</span>nih<span>&#46;</span>gov</a></div>
<div class="line">Slack (ECP-CANDLE workspace): &#64;Andrew Weisman</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="candle/candle.html" class="btn btn-neutral float-left" title="candle" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="contribute.html" class="btn btn-neutral float-right" title="How to Contribute" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, ANL.gov.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>